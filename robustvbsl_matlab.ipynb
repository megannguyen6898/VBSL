{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numdifftools as nd\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import multi_dot\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import invgamma\n",
    "from scipy.special import gamma\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import levy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATA FROM STABLE DISTRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE SUMMARY STATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.17408065,  0.19437762,  1.9275681 , -0.12839691])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate actual data\n",
    "actual_data = levy_stable.rvs(1.5, 0.5, scale = 1, loc = 0,  size=200)\n",
    "\n",
    "# Calculate summary stats of actual data\n",
    "stability = (np.percentile(actual_data, 95) - np.percentile(actual_data, 5)) / (np.percentile(actual_data, 75) - np.percentile(actual_data, 25))\n",
    "skewness = (np.percentile(actual_data, 95) + np.percentile(actual_data, 5) - 2 * np.percentile(actual_data, 50)) / (np.percentile(actual_data, 95) - np.percentile(actual_data, 5))\n",
    "scale = (np.percentile(actual_data, 75) - np.percentile(actual_data, 25)) / 1\n",
    "loc = np.mean(actual_data)\n",
    "\n",
    "actual_summary_statistics = np.array([stability, skewness, scale, loc])\n",
    "actual_summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND THE BEST THETA (COEFFICIENTS) USING VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVB:\n",
    "    def __init__(self, samples, actual_summary_statistics, learning_rate, threshold, l_threshold, adaptive_lr_1, adaptive_lr_2, t_w, Patience):\n",
    "        self.samples = samples\n",
    "        self.actual_summary_statistics = actual_summary_statistics\n",
    "        self.num_datasets = 100 # number of datasets\n",
    "        self.num_coeffs = np.shape(actual_summary_statistics)[0] # number of coeffs\n",
    "        self.lambda_dim = self.num_coeffs + int((self.num_coeffs * (self.num_coeffs + 1)) / 2)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.l_threshold = l_threshold\n",
    "        self.adaptive_lr_1 = adaptive_lr_1\n",
    "        self.adaptive_lr_2 = adaptive_lr_2\n",
    "        self.t_w = t_w\n",
    "        self.Patience = Patience\n",
    "\n",
    "    def summary_statistics(self, theta):\n",
    "        n_datasets = []\n",
    "        n_stability = []\n",
    "        n_skewness = []\n",
    "        n_scale = []\n",
    "        n_loc = []\n",
    "        for i in range(self.num_datasets):\n",
    "            n_datasets.append(levy_stable.rvs(theta[0], theta[1], scale = theta[2], loc = theta[3], size=200))\n",
    "            n_stability.append((np.percentile(n_datasets[i], 95) - np.percentile(n_datasets[i], 5)) / (np.percentile(n_datasets[i], 75) - np.percentile(n_datasets[i], 25)))\n",
    "            n_skewness.append((np.percentile(n_datasets[i], 95) + np.percentile(n_datasets[i], 5) - 2 * np.percentile(n_datasets[i], 50)) / (np.percentile(n_datasets[i], 95) - np.percentile(n_datasets[i], 5)))\n",
    "            n_scale.append((np.percentile(n_datasets[i], 75) - np.percentile(n_datasets[i], 25)) / theta[2])\n",
    "            n_loc.append(np.mean(n_datasets[i]))\n",
    "\n",
    "        summary_statistics = np.array([n_stability, n_skewness, n_scale, n_loc])\n",
    "        sample_mean = np.mean(summary_statistics, axis = 1)\n",
    "        sample_variance = np.cov(summary_statistics)\n",
    "\n",
    "        return sample_mean, sample_variance\n",
    "\n",
    "    # def prior(self, theta): \n",
    "    #     log_prior = - (self.num_coeffs/2) * np.log(2 * math.pi) - (self.num_coeffs/2) * np.log(100) - np.dot(theta.T, theta) / (2 * 100)\n",
    "    #     return log_prior\n",
    "\n",
    "    def prior(self, theta): \n",
    "        log_prior = multivariate_normal.logpdf(theta, cov= 100 * np.identity(self.num_coeffs))\n",
    "        \n",
    "        return log_prior\n",
    "\n",
    "    def unbiased_log_likelihood(self, theta):\n",
    "        sample_mean = self.summary_statistics(theta)[0]\n",
    "        sample_variance = self.summary_statistics(theta)[1]\n",
    "        diff_mean_s = self.actual_summary_statistics - sample_mean\n",
    "        part1 = diff_mean_s.T @ np.linalg.inv(sample_variance) @ diff_mean_s\n",
    "        u_est_log_likelihood = -1/2 * np.log(np.linalg.det(sample_variance)) - (self.num_datasets - self.num_coeffs - 2) / (2 * (self.num_datasets-1)) * part1\n",
    "        return u_est_log_likelihood\n",
    "\n",
    "    # def log_q(self, theta, mu, l):\n",
    "    #     log_q = - (self.num_coeffs/2) * np.log(2 * math.pi) - (1/2) * np.log(np.linalg.det(l)) - (1/2) * np.matmul(np.matmul(np.matmul(theta - mu, l), l.T), (theta - mu).T)\n",
    "    #     return log_q\n",
    "\n",
    "    def log_q(self, theta, mu, l):\n",
    "        log_q = multivariate_normal.logpdf(theta, mean = mu, cov= np.linalg.inv(l @ l.T))\n",
    "        return log_q\n",
    "\n",
    "    def gradient_log_q(self, theta, mu, l): #indep theta\n",
    "        gradient_log_q_mu = l @ l.T @ (theta - mu)\n",
    "        gradient_log_q_l = (np.diag(1 / np.diag(l)) - ((theta - mu) @ (theta - mu).T * l)).T[np.triu_indices(self.num_coeffs)] #use * because matmul gives scalar \n",
    "        gradient_log_q = np.array([gradient_log_q_mu, gradient_log_q_l], dtype=object)\n",
    "        return gradient_log_q\n",
    "\n",
    "    def control_variates(self, Flat_grad_log_q, Flat_grad_lb):\n",
    "        c = []\n",
    "        stacked_gradient_lb = np.stack(Flat_grad_lb)\n",
    "        stacked_gradient_log_q = np.stack(Flat_grad_log_q)\n",
    "        for i in range(self.lambda_dim):\n",
    "            sample_cov = np.cov((stacked_gradient_lb[:, i], stacked_gradient_log_q[:, i]))\n",
    "            c_i = sample_cov[0, 1] / sample_cov[1, 1]\n",
    "            c.append(c_i)\n",
    "        c_mu = np.array(c[0:self.num_coeffs])\n",
    "        c_vech_l = np.array(c[self.num_coeffs:])\n",
    "        return np.array([c_mu, c_vech_l], dtype = object)\n",
    "\n",
    "    def vb_posterior(self, stop):\n",
    "        # Initialize mu_0, L_0\n",
    "        #mu_0 = np.array([np.log((self.actual_summary_statistics[0]-1.1)/(2-self.actual_summary_statistics[0])), np.log((1+self.actual_summary_statistics[1])/(1-self.actual_summary_statistics[1])), np.log(self.actual_summary_statistics[2]), self.actual_summary_statistics[3]])\n",
    "        #mu_0 = np.array([1.5, 0.5, 3, 0])\n",
    "        #mu_0 = np.array([self.actual_summary_statistics[0], self.actual_summary_statistics[1], self.actual_summary_statistics[2], self.actual_summary_statistics[3]])\n",
    "        mu_0 = np.array([0] * self.num_coeffs)\n",
    "        l_0 = np.identity(self.num_coeffs) * 10\n",
    "        Sigma_0_inv = l_0 @ l_0.T\n",
    "        Sigma_0 = np.linalg.inv(Sigma_0_inv)\n",
    "        l_0_inv = np.linalg.inv(l_0)\n",
    "        ### Change ways to get vech(l0)\n",
    "        vech_l0 = Sigma_0.T[np.triu_indices(self.num_coeffs)]\n",
    "\n",
    "        lambda_0 = np.array([mu_0, vech_l0], dtype = object)\n",
    "        lambda_q = lambda_0\n",
    "        # List of Lambda\n",
    "        Lambda = [lambda_0]\n",
    "        # List of theta\n",
    "        Theta = []\n",
    "        Theta_t = []\n",
    "        # List of calculations of LB\n",
    "        LB_all = []\n",
    "        LB_Smoothed = []\n",
    "        patience = 0\n",
    "        # List of flattened gradients\n",
    "        Flattened_gradient_lb = []\n",
    "        Flattened_gradient_log_q = []\n",
    "        for t in tqdm(range(stop)):\n",
    "            if t == 0:        \n",
    "                # Draw samples of theta from  variational q\n",
    "                # List of gradients\n",
    "                Gradient_lb_init = []\n",
    "                lb_0 = []\n",
    "                theta_0_samples = multivariate_normal.rvs(mean = mu_0, cov = Sigma_0, size = self.samples)\n",
    "                for s in range(self.samples):\n",
    "                    # True params have been reparam into epsilon\n",
    "                    theta_tilde_0 = theta_0_samples[s]\n",
    "                    alpha_0 = (2 * np.exp(theta_tilde_0[0]) + 1.1) / (1 + np.exp(theta_tilde_0[0]))\n",
    "                    beta_0 = (np.exp(theta_tilde_0[1]) - 1) / (np.exp(theta_tilde_0[1]) + 1)\n",
    "                    gamma_0 = np.exp(theta_tilde_0[2])\n",
    "                    delta_0 = theta_tilde_0[3]\n",
    "                    theta_0 = np.array([alpha_0, beta_0, gamma_0, delta_0])\n",
    "                    \n",
    "                    Theta_t.append(theta_0)\n",
    "                    # Find gradient of LB\n",
    "                    h_lambda_init = self.prior(theta_tilde_0) + self.unbiased_log_likelihood(theta_0) - self.log_q(theta_tilde_0, mu_0, l_0)\n",
    "                    gradient_lb_init = self.gradient_log_q(theta_tilde_0, mu_0, l_0) * (h_lambda_init)\n",
    "                    Gradient_lb_init.append(gradient_lb_init)\n",
    "                    # Calculate control variates\n",
    "                    flattened_gradient_log_q = np.concatenate((self.gradient_log_q(theta_tilde_0, mu_0, l_0)[0], self.gradient_log_q(theta_tilde_0, mu_0, l_0)[1]), axis = None)\n",
    "                    Flattened_gradient_log_q.append(flattened_gradient_log_q)\n",
    "                    flattened_gradient_lb = np.concatenate((gradient_lb_init[0], gradient_lb_init[1]), axis = None)\n",
    "                    Flattened_gradient_lb.append(flattened_gradient_lb)\n",
    "                    # Calculate lower bound\n",
    "                    lb_0.append(h_lambda_init)\n",
    "                # Calculate control variates using all samples\n",
    "                c = self.control_variates(Flattened_gradient_log_q, Flattened_gradient_lb)\n",
    "                # Update lambda_q\n",
    "                self.g_init = np.mean(Gradient_lb_init, axis = 0)\n",
    "                # Gradient clipping\n",
    "                if np.linalg.norm(np.concatenate(self.g_init, axis = None)) > self.l_threshold:\n",
    "                    self.g_init = self.l_threshold * self.g_init / np.linalg.norm(np.concatenate(self.g_init, axis = None))\n",
    "                self.v_init = self.g_init ** 2\n",
    "                # Calculate lower bound\n",
    "                LB_all.append(np.mean(lb_0))\n",
    "                Theta.append(Theta_t)\n",
    "            if t > 0:\n",
    "                # From lambda_q find mu_q and L_q\n",
    "                mu_q = lambda_q[0]\n",
    "\n",
    "                ### Change ways to convert from vech_l0 to l0\n",
    "                l_q = np.zeros((self.num_coeffs, self.num_coeffs))\n",
    "                l_q[:, 0] = lambda_q[1][0:self.num_coeffs]\n",
    "                l_q[1:self.num_coeffs, 1] = lambda_q[1][4:7]\n",
    "                l_q[2:self.num_coeffs, 2] = lambda_q[1][7:9]\n",
    "                l_q[3:self.num_coeffs, 3] = lambda_q[1][9:10]\n",
    "\n",
    "                Sigma_q_inv = l_q @ l_q.T\n",
    "                Sigma_q =  np.linalg.inv(Sigma_q_inv)\n",
    "                l_q_inv =  np.linalg.inv(l_q)\n",
    "                # List of gradients\n",
    "                Gradient_lb = []\n",
    "                lb_t = []\n",
    "                theta_q_samples = multivariate_normal.rvs(mean = mu_q, cov = Sigma_q, size = self.samples)\n",
    "                for s in range(self.samples):\n",
    "                    theta_tilde_q = theta_q_samples[s]\n",
    "                    # Calculate theta from mu, l (lambda)\n",
    "                    alpha_q = (2 * np.exp(theta_tilde_q[0]) + 1.1) / (1 + np.exp(theta_tilde_q[0]))\n",
    "                    beta_q = (np.exp(theta_tilde_q[1]) - 1) / (np.exp(theta_tilde_q[1]) + 1)\n",
    "                    gamma_q = np.exp(theta_tilde_q[2])\n",
    "                    delta_q = theta_tilde_q[3]\n",
    "                    theta_q = np.array([alpha_q, beta_q, gamma_q, delta_q])\n",
    "                    \n",
    "                    Theta_t.append(theta_q)\n",
    "                    # Find gradient of LB\n",
    "                    h_lambda = self.prior(theta_tilde_q) + self.unbiased_log_likelihood(theta_q) - self.log_q(theta_tilde_q, mu_q, l_q)\n",
    "                    # Find gradient of LB\n",
    "                    gradient_lb = self.gradient_log_q(theta_tilde_q, mu_q, l_q) * (h_lambda - c)\n",
    "                    Gradient_lb.append(gradient_lb)\n",
    "                    # Calculate control variates\n",
    "                    Flattened_gradient_log_q[s] = np.concatenate((self.gradient_log_q(theta_tilde_q, mu_q, l_q)[0], self.gradient_log_q(theta_tilde_q, mu_q, l_q)[1]), axis = None)\n",
    "                    Flattened_gradient_lb[s] = np.concatenate((gradient_lb[0], gradient_lb[1]), axis = None)\n",
    "                    # Calc lower bound estimate\n",
    "                    lb_t.append(h_lambda)\n",
    "                # Update control variates\n",
    "                c = self.control_variates(Flattened_gradient_log_q, Flattened_gradient_lb)\n",
    "                # Calc gradient of h\n",
    "                g_t = np.mean(Gradient_lb, axis = 0)\n",
    "                # Gradient clipping\n",
    "                if np.linalg.norm(np.concatenate(g_t, axis = None)) > self.l_threshold:\n",
    "                    g_t = self.l_threshold * g_t / np.linalg.norm(np.concatenate(g_t, axis = None))\n",
    "                v_t = g_t ** 2\n",
    "\n",
    "                Theta.append(Theta_t)\n",
    "                #---- Update lambda\n",
    "                self.g_init = self.adaptive_lr_1 * self.g_init + (1 - self.adaptive_lr_1) * g_t\n",
    "                self.v_init = self.adaptive_lr_2 * self.v_init + (1 - self.adaptive_lr_2) * v_t\n",
    "\n",
    "                if t >= self.threshold:\n",
    "                    update_t = self.learning_rate * self.threshold / t\n",
    "                else:\n",
    "                    update_t = self.learning_rate\n",
    "\n",
    "                lambda_q = lambda_q + update_t * self.g_init / (self.v_init ** 0.5)\n",
    "                Lambda.append(lambda_q)\n",
    "                # Calculate lower bound\n",
    "                LB_all.append(np.mean(lb_t))\n",
    "\n",
    "                if t >= self.t_w:\n",
    "                    LB_smoothed = np.mean(LB_all[t - self.t_w + 1 : t])\n",
    "                    print(LB_smoothed)\n",
    "                    LB_Smoothed.append(LB_smoothed)\n",
    "                    if LB_smoothed < max(LB_Smoothed):\n",
    "                        patience += 1\n",
    "                        if patience > self.Patience:\n",
    "                            print(\"Stop at\", t)\n",
    "                            break\n",
    "\n",
    "                    best_LB_index = np.argmax(LB_Smoothed) + self.t_w - 1\n",
    "                    best_Lambda = Lambda[best_LB_index]\n",
    "                    best_Theta = Theta[best_LB_index]\n",
    "                    # best_Mu = best_Lambda[0]\n",
    "                    # best_L = np.zeros((self.num_coeffs, self.num_coeffs))\n",
    "                    # for i in (np.arange(self.num_coeffs)):\n",
    "                    #     index_from_tril = int((i+1) * (i+2) / 2 - 1)\n",
    "                    #     best_L[i][i] = best_Lambda[1][index_from_tril] \n",
    "\n",
    "        return LB_all, LB_Smoothed, best_LB_index, best_Lambda, best_Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN VB AND PRINT OUT VARIATIONAL PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "stop = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "vb = GVB(200, actual_summary_statistics, 0.001, 2500, 100, 0.9, 0.9, 50, 50)\n",
    "LB_estimate, smoothed_LB_estimate, best_LB_idx, best_lambda, best_theta = vb.vb_posterior(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT DENSITY PLOT OF ALL COEFFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df = pd.DataFrame(np.array(LB_estimate))\n",
    "plt.figure()\n",
    "lb_df.plot(title = 'Lower Bound Estimate', legend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df = pd.DataFrame(np.array(smoothed_LB_estimate))\n",
    "plt.figure()\n",
    "lb_df.plot(title = 'Smoothed Lower Bound Estimate', legend = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a0e653d523fbf32afc7f676d2e5612ee207b6f527b35184ac898d6e8f265e50"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
